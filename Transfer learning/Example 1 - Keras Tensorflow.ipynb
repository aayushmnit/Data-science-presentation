{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained VGG model for transfer learning\n",
    "\n",
    "In this example I will be using Keras with tensorflow backend. The data used is taken from cats and dogs classification problem hosted on Kaggle, but later been preprocessed for fast.ai course.  You can find the data [here](http://files.fast.ai/files/dogscats.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Importing required libraries\n",
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Input,Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline\n",
    "## Path of your data folder\n",
    "path = \"C:/Users/Aayush - Carlson/Downloads/fastai/deeplearning1/data/dogscats/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 0\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  134260544 \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aayush - Carlson\\Anaconda2\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"pr..., inputs=Tensor(\"im...)`\n"
     ]
    }
   ],
   "source": [
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "## Removing the last layer of the model\n",
    "model_vgg16_conv.layers.pop()\n",
    "model_vgg16_conv.outputs = [model_vgg16_conv.layers[-1].output]\n",
    "model_vgg16_conv.output_layers = [model_vgg16_conv.layers[-1]] # added this line in addition to zo7 solution\n",
    "model_vgg16_conv.layers[-1].outbound_nodes = []\n",
    "\n",
    "## Setting the model layers to be non trainable\n",
    "for layer in model_vgg16_conv.layers: layer.trainable=False\n",
    "    \n",
    "## Adding new layer to the model\n",
    "input = Input(shape=(224,224,3),name = 'image_input')\n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "x = Dense(2, activation='softmax', name='predictions')(output_vgg16_conv)\n",
    "\n",
    "#Adding new layer to the model\n",
    "my_model = Model(input=input, output=x)\n",
    "model_vgg16_conv.summary()\n",
    "my_model.summary()\n",
    "#my_model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "my_model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef get_batches(path, gen=ImageDataGenerator(), \\n                shuffle=True, batch_size=8, class_mode='categorical'):\\n    return gen.flow_from_directory(path,target_size=(224,224),class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using same preprocessing done on original VGG dataset, which is subtracting mean of original imagenet images from new images\n",
    "def preprocess_input(x):\n",
    "    x = x[:,:,::-1]\n",
    "    x[:,:,0] -= 103.939\n",
    "    x[:,:,1] -= 116.779\n",
    "    x[:,:,2] -= 123.68\n",
    "    return x[:,:,::-1]\n",
    "\n",
    "## Function for fetching data in batches\n",
    "def get_batches(path, gen=ImageDataGenerator(preprocessing_function = preprocess_input), \n",
    "                shuffle=True, batch_size=8, class_mode='categorical'):\n",
    "    return gen.flow_from_directory(path,target_size=(224,224),class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Getting data from the train and validation folder\n",
    "batch_size=8 ## Set this number as high as your gpu memory supports - Mine is a GTX 950M - 2 GB graphic card)\n",
    "batches = get_batches(path+'train', batch_size=batch_size) # Getting training images\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2) ## Getting validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 5922/23000 [======>.......................] - ETA: 4991s - loss: 0.1523 - acc: 0.9774"
     ]
    }
   ],
   "source": [
    "## Fitting the model\n",
    "my_model.fit_generator(batches, steps_per_epoch= batches.n ,validation_data = val_batches,validation_steps = val_batches.n, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see just in one epoch we were able to achieve >98% accuracy which would have won the Kaggle competition at the time it was launched. Transfer learning is a very powerful technique and should always be used unless the dataset is totally different from imagenet."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
